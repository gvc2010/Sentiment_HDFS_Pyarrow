{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a517f7f-2153-4ab7-85ca-6e4c7956cb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.csv as pc\n",
    "import pyarrow.hdfs as hdfs\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, split, explode\n",
    "from pyspark.sql import SparkSession\n",
    "from pyarrow import fs, hdfs\n",
    "\n",
    "hdfs.HadoopFileSystem(\"hdfs://namenode:9000\")\n",
    "conexao = fs.HadoopFileSystem(host=\"namenode\",port=9000)\n",
    "\n",
    "#conexao.create_dir('/guilherme')\n",
    "#conexao.delete_file('/somativa/sentiment140.csv')\n",
    "#conexao.delete_dir('/guilherme')\n",
    "conexao.get_file_info(fs.FileSelector('/', recursive=True))\n",
    "origem = '/home/jovyan/work/sentiment140.csv'\n",
    "destino = '/guilherme/sentiment140.csv'\n",
    "\n",
    "\n",
    "#conexao.copy_file(origem, destino)\n",
    "\n",
    "with conexao.open_output_stream(destino) as stream:\n",
    "    stream.write(open(origem,'rb').read())\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.csv(\"hdfs://namenode:9000/guilherme/sentiment140.csv\", header=True, inferSchema=True)\n",
    "df.show()\n",
    "\n",
    "# Tokenização\n",
    "df_tokens = df.withColumn('tokens', split(col('text'), '\\\\W+'))\n",
    "df_tokens = df_tokens.withColumn('tokens', explode(col('tokens')))\n",
    "\n",
    "# Contar as palavras mais frequentes\n",
    "df_most_frequent_tokens = df_tokens.groupBy('tokens').count().orderBy('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cb5822b-3714-4d3a-a4dd-4b8ed0bade35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_181/2085559598.py:12: FutureWarning: pyarrow.hdfs.HadoopFileSystem is deprecated as of 2.0.0, please use pyarrow.fs.HadoopFileSystem instead.\n",
      "  hdfs.HadoopFileSystem(\"hdfs://namenode:9000\")\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.csv as pc\n",
    "import pyarrow.hdfs as hdfs\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, split, explode\n",
    "from pyspark.sql import SparkSession\n",
    "from pyarrow import fs, hdfs\n",
    "\n",
    "hdfs.HadoopFileSystem(\"hdfs://namenode:9000\")\n",
    "conexao = fs.HadoopFileSystem(host=\"namenode\",port=9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7881436-3a71-4056-895c-9f16e0782b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "conexao.get_file_info(fs.FileSelector('/', recursive=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f105f838-fac2-4d05-8484-c8b05ede3851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O diretório foi excluido!: [Errno 2] Calling GetPathInfo for '/guilherme' failed. Detail: [errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Deletar o diretório se ele já existir\n",
    "try:\n",
    "    conexao.delete_dir('/guilherme')\n",
    "    conexao.delete_file('/guilherme/sentiment.csv')\n",
    "except Exception as e:\n",
    "    print(f\"O diretório foi excluido!: {e}\")\n",
    "    \n",
    "conexao.create_dir('/guilherme')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e3f8f79-fb87-4bdd-891d-1a3951c396a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<FileInfo for '/guilherme': type=FileType.Directory>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conexao.get_file_info(fs.FileSelector('/', recursive=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e83fa702-bf39-40b9-af58-8d58b817fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "origem = '/home/jovyan/work/sentiment140.csv'\n",
    "destino = '/guilherme/sentiment140.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47d3e26a-a222-4e25-ad82-f8a7b197640a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<FileInfo for '/guilherme': type=FileType.Directory>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conexao.get_file_info(fs.FileSelector('/', recursive=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7eb3321-891b-41fa-8102-3781c537a4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with conexao.open_output_stream(destino) as stream:\n",
    "    stream.write(open(origem,'rb').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff73e6c0-7ad5-480d-aa2a-92c0f263e387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<FileInfo for '/guilherme': type=FileType.Directory>,\n",
       " <FileInfo for '/guilherme/sentiment140.csv': type=FileType.File, size=44326223>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conexao.get_file_info(fs.FileSelector('/', recursive=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "632f8df1-1d79-4de0-917c-48f6df0cbbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------+---------------+--------------------+\n",
      "|target|       ids|                date|    flag|           user|                text|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+\n",
      "|     0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|     0|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|     0|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|     0|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|     0|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "|     0|1467811372|Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
      "|     0|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |\n",
      "|     0|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|     0|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|     0|1467812025|Mon Apr 06 22:20:...|NO_QUERY|        mimismo|@twittera que me ...|\n",
      "|     0|1467812416|Mon Apr 06 22:20:...|NO_QUERY| erinx3leannexo|spring break in p...|\n",
      "|     0|1467812579|Mon Apr 06 22:20:...|NO_QUERY|   pardonlauren|I just re-pierced...|\n",
      "|     0|1467812723|Mon Apr 06 22:20:...|NO_QUERY|           TLeC|@caregiving I cou...|\n",
      "|     0|1467812771|Mon Apr 06 22:20:...|NO_QUERY|robrobbierobert|@octolinz16 It it...|\n",
      "|     0|1467812784|Mon Apr 06 22:20:...|NO_QUERY|    bayofwolves|@smarrison i woul...|\n",
      "|     0|1467812799|Mon Apr 06 22:20:...|NO_QUERY|     HairByJess|@iamjazzyfizzle I...|\n",
      "|     0|1467812964|Mon Apr 06 22:20:...|NO_QUERY| lovesongwriter|Hollis' death sce...|\n",
      "|     0|1467813137|Mon Apr 06 22:20:...|NO_QUERY|       armotley|about to file taxes |\n",
      "|     0|1467813579|Mon Apr 06 22:20:...|NO_QUERY|     starkissed|@LettyA ahh ive a...|\n",
      "|     0|1467813782|Mon Apr 06 22:20:...|NO_QUERY|      gi_gi_bee|@FakerPattyPattz ...|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.csv(\"hdfs://namenode:9000/guilherme/sentiment140.csv\", header=True, inferSchema=True, sep=';')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e0080b6-0c8f-475f-9e40-2e25360607c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                     |\n",
      "+---------------------------------------------------------------------------------------------------------+\n",
      "|@switchfoot httptwitpiccom2y1zl  awww thats a bummer  you shoulda got david carr of third day to do it d |\n",
      "|is upset that he cant update his facebook by texting it and might cry as a result  school today also blah|\n",
      "|@kenichan i dived many times for the ball managed to save 50  the rest go out of bounds                  |\n",
      "|my whole body feels itchy and like its on fire                                                           |\n",
      "|@nationwideclass no its not behaving at all im mad why am i here because i cant see you all over there   |\n",
      "+---------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+------+\n",
      "|word| count|\n",
      "+----+------+\n",
      "|    |236865|\n",
      "|   i|181426|\n",
      "|  to|127227|\n",
      "| the|104178|\n",
      "|  my| 74140|\n",
      "|   a| 72945|\n",
      "| and| 60990|\n",
      "|  is| 51644|\n",
      "|  it| 47605|\n",
      "|  in| 46196|\n",
      "+----+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------------+-----+\n",
      "|            word|count|\n",
      "+----------------+-----+\n",
      "|               @| 2426|\n",
      "|     @mileycyrus|  699|\n",
      "|       @tommcfly|  666|\n",
      "|       @ddlovato|  418|\n",
      "|@jonathanrknight|  258|\n",
      "|  @taylorswift13|  200|\n",
      "|   @mitchelmusso|  195|\n",
      "|    @davidarchie|  192|\n",
      "|  @jonasbrothers|  174|\n",
      "| @donniewahlberg|  161|\n",
      "+----------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------+-----+\n",
      "|word         |count|\n",
      "+-------------+-----+\n",
      "|#fb          |376  |\n",
      "|#fail        |143  |\n",
      "|#asot400     |135  |\n",
      "|#bgt         |112  |\n",
      "|#            |111  |\n",
      "|#e3          |76   |\n",
      "|#followfriday|72   |\n",
      "|#1           |61   |\n",
      "|#ontd        |59   |\n",
      "|#2           |58   |\n",
      "+-------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lower, regexp_replace, split, explode\n",
    "\n",
    "# Seleciona a coluna de texto e guarda elas no dataframe tweets\n",
    "tweets_df = df.select(\"text\")\n",
    "\n",
    "# Limpa o texto para facilitar entendimento \n",
    "tweets_df_cleaned = tweets_df.withColumn(\"text\", lower(regexp_replace(col(\"text\"), \"[^a-zA-Z0-9@#\\\\s]\", \"\")))\n",
    "# Mostra os primeiros registros para verificar a limpeza\n",
    "tweets_df_cleaned.show(5, truncate=False)\n",
    "\n",
    "# Tokenizar os tweets\n",
    "words_df = tweets_df_cleaned.withColumn(\"word\", explode(split(col(\"text\"), \"\\\\s+\")))\n",
    "# Conta a frequência de cada palavra\n",
    "word_freq_df = words_df.groupBy(\"word\").count().orderBy(col(\"count\").desc())\n",
    "# Mostra as palavras mais frequentes\n",
    "word_freq_df.show(10)\n",
    "\n",
    "# Separando usuários mencionados (que começam com @) e os coloca em um dataframe \n",
    "mentions_df = words_df.filter(words_df.word.startswith(\"@\"))\n",
    "# Conta a frequência de usuário mencionado\n",
    "mentions_freq_df = mentions_df.groupBy(\"word\").count().orderBy(col(\"count\").desc())\n",
    "# Mostra as menções mais frequentes\n",
    "mentions_freq_df.show(10)\n",
    "\n",
    "# Separa as hashtags e colocas elas em um dataframe \n",
    "hashtags_df = words_df.filter(words_df.word.startswith(\"#\"))\n",
    "# Conta as hashtags mais frequentes \n",
    "hashtags_freq_df = hashtags_df.groupBy(\"word\").count().orderBy(col(\"count\").desc())\n",
    "# Mostra as hashtags mais frequentes\n",
    "hashtags_freq_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002984c5-f3aa-4b8f-96c5-004d8a3422c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
